# RPA Scraping System

Sistema de coleta de dados de mÃºltiplas fontes web com gerenciamento de jobs atravÃ©s de filas de mensagens e API REST.

[![Tests](https://img.shields.io/badge/tests-59%20passing-brightgreen)](tests/)
[![Python](https://img.shields.io/badge/python-3.11%2B-blue)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green)](https://fastapi.tiangolo.com/)
[![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-2.0+-orange)](https://www.sqlalchemy.org/)

> ğŸ“‹ **DocumentaÃ§Ã£o Completa**:
> - [REQUIREMENTS.md](REQUIREMENTS.md) - EspecificaÃ§Ãµes tÃ©cnicas originais
> - [CONFIGURATION.md](CONFIGURATION.md) - Guia de configuraÃ§Ã£o completo
> - [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) - Detalhes da refatoraÃ§Ã£o recente

## ğŸ¯ VisÃ£o Geral

Sistema completo de scraping web com arquitetura moderna e assÃ­ncrona:

- âœ… **API REST** com FastAPI (10 endpoints)
- âœ… **Processamento assÃ­ncrono** com RabbitMQ
- âœ… **PersistÃªncia** em PostgreSQL com SQLAlchemy
- âœ… **Scrapers** estÃ¡ticos (BeautifulSoup) e dinÃ¢micos (Selenium)
- âœ… **Testes** completos (59 testes unitÃ¡rios e de integraÃ§Ã£o)
- âœ… **CI/CD** com GitHub Actions
- âœ… **Type hints** em 100% do cÃ³digo
- âœ… **Docker** e docker-compose prontos para uso

### ğŸ“Š Status do Projeto

| Categoria | Status |
|-----------|--------|
| **Testes** | âœ… 59/59 passando |
| **Cobertura** | âœ… Completa |
| **Type Hints** | âœ… 100% |
| **DocumentaÃ§Ã£o** | âœ… Completa |
| **CI/CD** | âœ… Configurado |
| **Deprecations** | âœ… 1 warning (biblioteca externa) |

## ğŸš€ Quick Start

### Com Docker (Recomendado)

```bash
# 1. Clonar repositÃ³rio
git clone <repo-url>
cd teste-tecnico-senior-rpa

# 2. Configurar ambiente
cp .env.example .env
# Editar .env com suas credenciais (ou usar valores padrÃ£o do docker-compose)

# 3. Iniciar todos os serviÃ§os
docker-compose up --build

# 4. Acessar aplicaÃ§Ã£o
# API: http://localhost:8000
# Docs: http://localhost:8000/docs
# RabbitMQ: http://localhost:15672 (guest/guest)
```

### Sem Docker

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt

# 2. Configurar variÃ¡veis de ambiente
export DATABASE_URL="postgresql://user:pass@localhost:5432/dbname"
export RABBITMQ_HOST="localhost"
# ... outras variÃ¡veis

# 3. Inicializar banco
python -c "from app.core.database import init_db; init_db()"

# 4. Iniciar API
uvicorn app.main:app --reload

# 5. Iniciar Worker (em outro terminal)
python -m app.worker
```

## ğŸ“š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚â”€â”€â”€â”€â–¶â”‚  RabbitMQ   â”‚â”€â”€â”€â”€â–¶â”‚   Workers   â”‚
â”‚    (API)    â”‚     â”‚   (Queue)   â”‚     â”‚  (Crawlers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  PostgreSQL â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚    (Data)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Trabalho

1. **Cliente** faz POST para `/crawl/hockey` ou `/crawl/oscar`
2. **API** cria job no banco e publica mensagem no RabbitMQ
3. **Worker** consome mensagem e executa scraper
4. **Scraper** coleta dados e salva no PostgreSQL
5. **Cliente** consulta status via `/jobs/{id}` e resultados via `/jobs/{id}/results`

## ğŸ“– API Endpoints

### Scraping (AssÃ­ncrono)

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `POST` | `/crawl/hockey` | Agenda coleta de dados de Hockey |
| `POST` | `/crawl/oscar` | Agenda coleta de dados de Oscar |
| `POST` | `/crawl/all` | Agenda ambas as coletas |

### Gerenciamento de Jobs

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/jobs` | Lista todos os jobs |
| `GET` | `/jobs/{job_id}` | Detalhes de um job especÃ­fico |
| `GET` | `/jobs/{job_id}/results` | Resultados coletados por um job |

### Consulta de Dados

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/results/hockey` | Todos os dados de Hockey coletados |
| `GET` | `/results/oscar` | Todos os dados de Oscar coletados |

### Exemplo de Uso

```bash
# 1. Agendar scraping
curl -X POST http://localhost:8000/crawl/hockey
# Response: {"job_id": 1, "message": "...", "status": "pending"}

# 2. Verificar status
curl http://localhost:8000/jobs/1
# Response: {"id": 1, "type": "hockey", "status": "running", ...}

# 3. Obter resultados
curl http://localhost:8000/jobs/1/results
# Response: {"job": {...}, "hockey_data": [...], "oscar_data": []}
```

## ğŸ—ï¸ Estrutura do Projeto

```
teste-tecnico-senior-rpa/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/              # âš™ï¸ Infraestrutura
â”‚   â”‚   â”œâ”€â”€ config.py      # ConfiguraÃ§Ãµes (Pydantic Settings)
â”‚   â”‚   â”œâ”€â”€ database.py    # SQLAlchemy engine e sessÃµes
â”‚   â”‚   â””â”€â”€ rabbitmq.py    # ConexÃ£o RabbitMQ
â”‚   â”œâ”€â”€ static_scraper/    # ğŸŒ Scrapers estÃ¡ticos (BeautifulSoup)
â”‚   â”‚   â””â”€â”€ hockey.py      # Scraper de Hockey
â”‚   â”œâ”€â”€ dynamic_scraper/   # ğŸ”„ Scrapers dinÃ¢micos (Selenium)
â”‚   â”‚   â””â”€â”€ oscar.py       # Scraper de Oscar
â”‚   â”œâ”€â”€ models.py          # ğŸ—„ï¸ SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py         # âœ… Pydantic schemas (validaÃ§Ã£o)
â”‚   â”œâ”€â”€ main.py            # ğŸš€ FastAPI app
â”‚   â””â”€â”€ worker.py          # ğŸ‘· RabbitMQ consumer
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/              # ğŸ§ª Testes unitÃ¡rios (25)
â”‚   â”‚   â”œâ”€â”€ test_parsers.py
â”‚   â”‚   â””â”€â”€ test_schemas.py
â”‚   â””â”€â”€ integration/       # ğŸ”— Testes de integraÃ§Ã£o (34)
â”‚       â”œâ”€â”€ test_api.py
â”‚       â””â”€â”€ test_worker.py
â”œâ”€â”€ .github/workflows/     # ğŸ”„ CI/CD
â”‚   â””â”€â”€ ci.yml             # GitHub Actions
â”œâ”€â”€ docker-compose.yml     # ğŸ³ OrquestraÃ§Ã£o
â”œâ”€â”€ Dockerfile             # ğŸ“¦ Imagem da aplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt       # ğŸ“‹ DependÃªncias
â””â”€â”€ .env.example          # ğŸ” Template de configuraÃ§Ã£o
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Com cobertura
pytest --cov=app tests/

# Apenas unitÃ¡rios
pytest tests/unit/

# Apenas integraÃ§Ã£o
pytest tests/integration/

# Modo verbose
pytest -v
```

**Status**: âœ… 59/59 testes passando

## ğŸ”§ Desenvolvimento

### Linting e FormataÃ§Ã£o

```bash
# Verificar cÃ³digo
ruff check app/ tests/

# Formatar cÃ³digo
black app/ tests/

# Type checking
mypy app/ --ignore-missing-imports
```

### Ambiente Nix (Linux)

```bash
# Permitir direnv
direnv allow

# Ambiente serÃ¡ carregado automaticamente
# Veja flake.nix para detalhes
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` baseado em `.env.example`:

```env
# Database
DATABASE_URL=postgresql://rpa_user:rpa_password@postgres:5432/rpa_db

# RabbitMQ
RABBITMQ_HOST=rabbitmq
RABBITMQ_PORT=5672
RABBITMQ_USER=rpa_user
RABBITMQ_PASSWORD=rpa_password

# URLs dos sites (jÃ¡ configuradas)
HOCKEY_URL=https://www.scrapethissite.com/pages/forms/
OSCAR_URL=https://www.scrapethissite.com/pages/ajax-javascript/

# Scrapers
SELENIUM_HEADLESS=true
SCRAPER_DELAY=1.0
```

> ğŸ“– **Veja [CONFIGURATION.md](CONFIGURATION.md)** para referÃªncia completa

### GitHub Secrets (ProduÃ§Ã£o)

Configure em: `Settings` â†’ `Secrets and variables` â†’ `Actions`

```yaml
DATABASE_URL: <postgresql://...>
RABBITMQ_HOST: <host>
RABBITMQ_PORT: <5672>
RABBITMQ_USER: <user>
RABBITMQ_PASSWORD: <password>
```

## ğŸ“¦ Stack TecnolÃ³gica

| Tecnologia | VersÃ£o | Uso |
|------------|--------|-----|
| **Python** | 3.11+ | Linguagem base |
| **FastAPI** | 0.109+ | Framework web |
| **SQLAlchemy** | 2.0+ | ORM |
| **PostgreSQL** | 15+ | Banco de dados |
| **RabbitMQ** | 3.12+ | Message broker |
| **Pydantic** | 2.5+ | ValidaÃ§Ã£o de dados |
| **BeautifulSoup4** | 4.12+ | Scraping estÃ¡tico |
| **Selenium** | 4.16+ | Scraping dinÃ¢mico |
| **Docker** | 24+ | ContainerizaÃ§Ã£o |
| **pytest** | 7.4+ | Framework de testes |

## ğŸ¯ Features Implementadas

### âœ… Requisitos Funcionais

- [x] Coleta de dados de duas fontes distintas
- [x] EstratÃ©gias diferentes de scraping (estÃ¡tico + dinÃ¢mico)
- [x] Sistema de filas com RabbitMQ
- [x] PersistÃªncia em PostgreSQL
- [x] API REST assÃ­ncrona
- [x] Testes automatizados (unitÃ¡rios + integraÃ§Ã£o)
- [x] ContainerizaÃ§Ã£o com Docker
- [x] CI/CD com GitHub Actions

### âœ… Requisitos TÃ©cnicos

- [x] FastAPI para API
- [x] Pydantic para validaÃ§Ã£o
- [x] SQLAlchemy como ORM
- [x] PostgreSQL como banco
- [x] RabbitMQ para filas
- [x] BeautifulSoup para pÃ¡ginas estÃ¡ticas
- [x] Selenium para pÃ¡ginas dinÃ¢micas
- [x] Docker + Docker Compose
- [x] GitHub Actions para CI/CD

### âœ… Qualidade de CÃ³digo

- [x] Type hints em 100% do cÃ³digo
- [x] Docstrings completas
- [x] SOLID principles
- [x] DRY (Don't Repeat Yourself)
- [x] SeparaÃ§Ã£o de responsabilidades
- [x] Error handling robusto
- [x] Logging profissional
- [x] Zero cÃ³digo duplicado

## ğŸ”„ Melhorias Recentes

Veja [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) para detalhes completos.

### Highlights

- âœ… MigraÃ§Ã£o para SQLAlchemy 2.0 (DeclarativeBase)
- âœ… AtualizaÃ§Ã£o para FastAPI lifespan events
- âœ… Sistema de logging profissional
- âœ… RefatoraÃ§Ã£o de cÃ³digo duplicado (-60 linhas)
- âœ… InicializaÃ§Ã£o automÃ¡tica do banco
- âœ… Error handling melhorado
- âœ… ReduÃ§Ã£o de warnings de 4 para 1

## ğŸ“ Roadmap

### Implementado âœ…

- [x] Arquitetura bÃ¡sica
- [x] API REST completa
- [x] Sistema de workers
- [x] Scrapers funcio nais
- [x] Testes completos
- [x] CI/CD configurado
- [x] RefatoraÃ§Ã£o e modernizaÃ§Ã£o

### PrÃ³ximos Passos ğŸš§

- [ ] Alembic para migrations
- [ ] Cache layer (Redis)
- [ ] Rate limiting
- [ ] API authentication
- [ ] Monitoring (Sentry, Prometheus)
- [ ] Deploy em produÃ§Ã£o

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como teste tÃ©cnico.

## ğŸ“§ Contato

Para dÃºvidas ou sugestÃµes:
- Email: ti@bpcreditos.com.br
- Email: gabrielpelizzaro@gmail.com

---

**Desenvolvido com â¤ï¸ usando FastAPI, RabbitMQ e PostgreSQL**
